import asyncio
import json
from scraper import PracujScraper
from curl_cffi.requests import AsyncSession

async def main():
    # 1. Inicjalizacja
    scraper = PracujScraper()
    
    # Twoje keywords dla Job Hunter
    keywords = [
        "dyrektor sprzedaÅ¼y",
        "commercial director",
        "head of sales",
        "dyrektor handlowy"
    ]
    
    all_results = []
    
    print(f"ğŸ” Job Hunter - Rozpoczynam scraping dla: {keywords}\n")

    # 2. Sesja asynchroniczna
    async with AsyncSession() as client:
        # Uruchamiamy zadania rÃ³wnolegle (1 strona na keyword)
        tasks = [scraper.scrape_keyword(client, kw, max_pages=1) for kw in keywords]
        
        results = await asyncio.gather(*tasks)
        
        for r in results:
            all_results.extend(r)

    # 3. WyÅ›wietlenie wynikÃ³w
    print(f"\nâœ… Scraping zakoÅ„czony. Znaleziono Å‚Ä…cznie: {len(all_results)} ofert.\n")
    
    if all_results:
        # WyÅ›wietlamy wszystkie wyniki w formacie tabelarycznym
        print("=" * 100)
        print("RAPORT: OFERTY PRACY (OSTATNIE 7 DNI)")
        print("=" * 100)
        
        for idx, offer in enumerate(all_results, 1):
            print(f"\n{idx}. {offer['Title']}")
            print(f"   ğŸ¢ Firma: {offer['Company']}")
            print(f"   ğŸ“ Lokalizacja: {offer['Location']}")
            print(f"   ğŸ’° Wynagrodzenie: {offer['Salary']}")
            print(f"   ğŸ”— Link: {offer['Link']}")
            print("-" * 100)
    else:
        print("âŒ Nie znaleziono Å¼adnych wynikÃ³w.")

if __name__ == "__main__":
    asyncio.run(main())
