import asyncio
import json
from scraper import PracujScraper
from curl_cffi.requests import AsyncSession

async def main():
    # 1. Inicjalizacja
    scraper = PracujScraper()
    keywords = ["dyrektor fmcg", "head of retail", "dyrektor zakup√≥w"] # Job Hunter keywords
    all_results = []
    
    print(f"üîç Job Hunter - Rozpoczynam scraping dla: {keywords}")

    # 2. Sesja asynchroniczna
    async with AsyncSession() as client:
        # Uruchamiamy zadania r√≥wnolegle
        tasks = [scraper.scrape_keyword(client, kw, max_pages=1) for kw in keywords]
        
        results = await asyncio.gather(*tasks)
        
        for r in results:
            all_results.extend(r)

    # 3. Wy≈õwietlenie wynik√≥w
    print(f"\n‚úÖ Scraping zako≈Ñczony. Znaleziono ≈ÇƒÖcznie: {len(all_results)} ofert.")
    
    if all_results:
        # Wy≈õwietlamy wszystkie wyniki w formacie tabelarycznym
        print("\n--- RAPORT: OFERTY PRACY ---")
        for idx, offer in enumerate(all_results, 1):
            print(f"\n{idx}. {offer['Title']}")
            print(f"   Firma: {offer['Company']}")
            print(f"   Lokalizacja: {offer['Location']}")
            print(f"   Wynagrodzenie: {offer['Salary']}")
            print(f"   Link: {offer['Link']}")
    else:
        print("‚ùå Nie znaleziono ≈ºadnych wynik√≥w. Sprawd≈∫, czy portal nie zmieni≈Ç struktury lub czy nie masz blokady IP.")

if __name__ == "__main__":
    asyncio.run(main())
